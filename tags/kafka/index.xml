<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafka on Alex Mitelman</title>
    <link>https://mitelman.engineering/tags/kafka/</link>
    <description>Recent content in kafka on Alex Mitelman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://mitelman.engineering/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>System Design Weekly 017: July - August 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/017/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/017/</guid>
      <description>Highlights How WhatsApp enables multi-device capability WhatsApp phone client was previously a source of truth. If someone wanted to use WhatsApp on another device, the messages would be transferred through the smartphone app. If the smartphone battery was drained, such a companion app would not be able to work. The smartphone kept the data.
WhatsApp now allows connecting 4 additional devices that are independent of the smartphone. Each device gets an identity key.</description>
    </item>
    
    <item>
      <title>System Design Weekly 016: July 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/016/</link>
      <pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/016/</guid>
      <description>Highlights DoorDash: Building Faster Indexing with Apache Kafka and Elasticsearch The DoorDash team faced an issue of a very long time for updating the search index. They&amp;rsquo;ve built a search system relying on open source technologies. It uses Kafka as a message queue and for data storage, Flink for data transformation, and sending data to Elasticsearch. A reliable indexing system would ensure that changes in stores and items are reflected in the search index in real-time.</description>
    </item>
    
    <item>
      <title>System Design Weekly 014: July 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/014/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/014/</guid>
      <description>I came across the word &amp;ldquo;exabyte&amp;rdquo; three times in just one today. Previously I didn&amp;rsquo;t even know this word exists. So 1 exabyte is 1,000 petabytes, or 1 exabyte is 1,000,000 terabytes. Companies operate at a scale of millions of terabytes now.
&amp;ldquo;Apple is apparently Google’s largest customer now, followed by ByteDance (parent company of the TikTok app). Apple holds 8 exabytes of data with Google Cloud, ByteDance is in the region of 500 petabytes — 16x less.</description>
    </item>
    
    <item>
      <title>System Design Weekly 013: June 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/013/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/013/</guid>
      <description>Highlights Learn how Dream11, the World’s largest fantasy sports platform, scale their social network with Amazon Neptune and Amazon ElastiCache Dream11 is a fantasy sports platform that has social network features. The team evaluated different graph database solutions for the social network service and chose Amazon Neptune after a load/stress PoC. Dream11 is already operating within AWS infrastructure so including a fully managed graph DB into the VPC was one of the factors.</description>
    </item>
    
    <item>
      <title>System Design Weekly 011: May 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/011/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/011/</guid>
      <description>Highlights Pinterest: Shallow Mirror Kafka MirrorMaker is a tool to replicate Kafka clusters across different regions. Data from different Source Brokers is transferred to MirrorMaker which then sends this data to Destination Brokers in other regions. Pinterest started experiencing scalability issues at some point.
Monitoring showed some CPU and memory spikes. During the investigation, it became apparent that most of the CPU time was spent on message decompression and recompression. Memory consumption was often 2-10 times bigger than the actual data being sent.</description>
    </item>
    
    <item>
      <title>System Design Weekly 005: April 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/005/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/005/</guid>
      <description>Highlights Kiwi.com: Nonstop Operations with Scylla Even Through the OVHcloud Fire Fire on French OVHcloud affected four datacenters: SBG2 was destroyed, SBG1 adjacent rooms were partially on fire, SBG3 and SBG4 were switched off to fight the fire. Overall, 3.6 million websites were affected, including banks and mail servers.
Kiwi.com uses Scylla - NoSQL database, as a highly available and resilient solution. Their monitoring system detected spikes as nodes went down but later other OVHcloud datacenters took over the requests.</description>
    </item>
    
    <item>
      <title>System Design Weekly 002: March 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/002/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/002/</guid>
      <description>Highlights Cloudflare: The benefits of serving stale DNS entries when using Consul Cloudflare faced an issue with long latencies for DNS responses in certain parts of the world. In addition DNS over TLS is also a factor. They use Unbound as a DNS resolver. For a better failover, they set 30 seconds TTL for such responses.
There are two options to solve this problem. The first is prefetching. This means that on each request TTL is checked.</description>
    </item>
    
  </channel>
</rss>
