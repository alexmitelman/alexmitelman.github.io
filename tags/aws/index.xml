<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on Alex Mitelman</title>
    <link>https://mitelman.engineering/tags/aws/</link>
    <description>Recent content in aws on Alex Mitelman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://mitelman.engineering/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>System Design Weekly 011: May 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/011/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/011/</guid>
      <description>Highlights Pinterest: Shallow Mirror Kafka MirrorMaker is a tool to replicate Kafka clusters across different regions. Data from different Source Brokers is transferred to MirrorMaker which then sends this data to Destination Brokers in other regions. Pinterest started experiencing scalability issues at some point.
Monitoring showed some CPU and memory spikes. During the investigation, it became apparent that most of the CPU time was spent on message decompression and recompression. Memory consumption was often 2-10 times bigger than the actual data being sent.</description>
    </item>
    
    <item>
      <title>System Design Weekly 006: April 2021</title>
      <link>https://mitelman.engineering/system-design-weekly/006/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mitelman.engineering/system-design-weekly/006/</guid>
      <description>Highlights GitHub: How we scaled the API with a sharded, replicated rate limiter in Redis GitHub API has a limit on API calls per key. Such keys were stored in Memcached along with their reset_at value and number of calls. Memcached was also used for application caching purposes.
Such a solution works well but harder to scale. It was decided to have one Memcached per datacenter, in which case clients can face some issues if requests hit different datacenters.</description>
    </item>
    
  </channel>
</rss>
